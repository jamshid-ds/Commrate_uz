# -*- coding: utf-8 -*-
"""Commrate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ixWdPzkxmq4kRRHqjCb60rC45N3T9w3e

#DOCUMENTATION

#LIBRARIES
"""

import pandas as pd
import numpy as np

import re

from transformers import BertTokenizerFast, AutoTokenizer

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

"""#TOKINEZER"""

tokenizer = BertTokenizerFast.from_pretrained(
    "bert-base-uncased"
)

with open("positive10kUZ.txt", "r") as f:
  text_pos10 = f.read()

with open("negative10kUZ.txt", "r") as f:
  text_neg10 = f.read()

with open("UZ_negative.txt", "r") as f:
  text_neg = f.read()

with open("UZ_positive.txt", "r") as f:
  text_pos = f.read()

text = text_pos10 + text_neg10 + text_neg + text_pos
text=text.replace("\n"," ")
text = re.sub(r'[^a-zA-Z0-9 .,]', '', text)


text = text.lower()

def training_corpus():
  for i in range(0, len(text), 1000):
    yield text[i:i+1000]

old_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
training_corpus = training_corpus()

new_tokinizer = old_tokenizer.train_new_from_iterator(training_corpus, 15000)

vectorizer = CountVectorizer(tokenizer=new_tokinizer.tokenize)

"""#DATA REVIEW"""

data_pre = pd.read_csv("dataset.csv")

data_pre.sample(10)

"""#ML AND METRICS"""

import matplotlib.pyplot as plt
X = data_pre["text"]
y = data_pre["label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

X_train_vectorized = vectorizer.fit_transform(X_train).astype(np.float32)
X_test_vectorized = vectorizer.transform(X_test).astype(np.float32)

model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1)
model.fit(X_train_vectorized, y_train)

y_pred = model.predict(X_test_vectorized)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.show()

model_rf = RandomForestClassifier(n_estimators=50)
model_rf.fit(X_train_vectorized, y_train)

y_pred = model_rf.predict(X_test_vectorized)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.show()

model_log = LogisticRegression()
model_log.fit(X_train_vectorized, y_train)

y_pred = model_log.predict(X_test_vectorized)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.show()